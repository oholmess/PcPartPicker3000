{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating an analytics type application where the user can explore the dataset provided with an interactive UI. One page in our application will allow the user to choose different features of a PC, Laptop, Or Partially built PC in order to provide the predicted price based on certain specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Valentina G\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Valentina G\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Valentina G\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lightgbm) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lightgbm) (1.15.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Valentina G\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xgboost) (1.15.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Valentina G\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Valentina G\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\valentina g\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Valentina G\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Ensure the executable path is quoted to handle spaces\n",
    "!\"{sys.executable}\" -m pip install --upgrade pandas\n",
    "!\"{sys.executable}\" -m pip install --upgrade numpy\n",
    "!\"{sys.executable}\" -m pip install --upgrade scikit-learn\n",
    "!\"{sys.executable}\" -m pip install --upgrade lightgbm\n",
    "!\"{sys.executable}\" -m pip install --upgrade xgboost\n",
    "!\"{sys.executable}\" -m pip install --upgrade matplotlib\n",
    "!\"{sys.executable}\" -m pip install --upgrade seaborn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training for LAPTOPS with User-Specified Features ---\n",
      "User-specified features for LAPTOP model training: ['ram_memoria_gb', 'ram_frecuencia_de_la_memoria_mhz', 'ram_tipo', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador', 'procesador_frecuencia_turbo_max_ghz', 'grafica_tarjeta', 'pantalla_resolucion_pixeles', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi', 'alimentacion_vatios_hora', 'procesador_numero_nucleos', 'grafic_memoria', 'sonido_sistema_de_altavoces', 'alimentacion_wattage_binned']\n",
      "Loading data...\n",
      "Desktop PC data loaded successfully.\n",
      "Dropped columns from df_laptop (if they existed): ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
      "\n",
      "Warning: The following specified features were NOT found in df_laptop and will be excluded: ['grafic_memoria', 'alimentacion_wattage_binned']\n",
      "\n",
      "Using the following available features for LAPTOP training: ['ram_memoria_gb', 'ram_frecuencia_de_la_memoria_mhz', 'ram_tipo', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador', 'procesador_frecuencia_turbo_max_ghz', 'grafica_tarjeta', 'pantalla_resolucion_pixeles', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi', 'alimentacion_vatios_hora', 'procesador_numero_nucleos', 'sonido_sistema_de_altavoces']\n",
      "Categorical features for this LAPTOP model: ['ram_tipo', 'procesador', 'grafica_tarjeta', 'pantalla_resolucion_pixeles', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi', 'sonido_sistema_de_altavoces']\n",
      "Numerical features for this LAPTOP model: ['ram_memoria_gb', 'ram_frecuencia_de_la_memoria_mhz', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador_frecuencia_turbo_max_ghz', 'alimentacion_vatios_hora', 'procesador_numero_nucleos']\n",
      "\n",
      "Fitting GridSearchCV with user-specified features for LAPTOPS...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "GridSearchCV fitting process complete. Extracting best parameters...\n",
      "\n",
      "Best parameters found by GridSearchCV (User-Specified Features for LAPTOPS):\n",
      "{'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 10, 'regressor__min_child_samples': 20, 'regressor__n_estimators': 500, 'regressor__num_leaves': 70, 'regressor__reg_alpha': 0.001, 'regressor__reg_lambda': 0.5, 'regressor__subsample': 0.8}\n",
      "\n",
      "LAPTOP model training complete using best estimator from GridSearchCV (User-Specified Features).\n",
      "\n",
      "Metrics on the LAPTOP TEST Set (User-Specified Features):\n",
      "Mean Squared Error (MSE): 441416.2237\n",
      "Root Mean Squared Error (RMSE): 664.3916\n",
      "R-squared: 0.7406\n",
      "\n",
      "Metrics on the LAPTOP TRAINING Set (User-Specified Features):\n",
      "Mean Squared Error (MSE): 218427.2776\n",
      "Root Mean Squared Error (RMSE): 467.3620\n",
      "R-squared: 0.8569\n",
      "\n",
      "--- LAPTOP Model Training with User-Specified Features Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Valentina G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Valentina G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb # <--- Added XGBoost import\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.ensemble import GradientBoostingRegressor # Kept if you might switch back\n",
    "from sklearn.metrics import mean_squared_error, r2_score # make_scorer is available if needed\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb # Kept for the feature selection part, can be removed if FS model also changes\n",
    "\n",
    "print(\"\\n--- Starting Model Training for LAPTOPS with User-Specified Features ---\")\n",
    "\n",
    "# 1. Define the user-specified feature columns (same as before)\n",
    "user_feature_columns_laptop = [\n",
    "    \"ram_memoria_gb\",\n",
    "    \"ram_frecuencia_de_la_memoria_mhz\",\n",
    "    \"ram_tipo\",\n",
    "    \"disco_duro_capacidad_de_memoria_ssd_gb\",\n",
    "    \"procesador\",\n",
    "    \"procesador_frecuencia_turbo_max_ghz\",\n",
    "    \"grafica_tarjeta\",\n",
    "    \"pantalla_resolucion_pixeles\", # This is highly relevant for laptops\n",
    "    \"sistema_operativo_sistema_operativo\",\n",
    "    \"comunicaciones_estandar_wifi\",\n",
    "    \"alimentacion_vatios_hora\",\n",
    "    \"procesador_numero_nucleos\",\n",
    "    \"grafic_memoria\",\n",
    "    \"sonido_sistema_de_altavoces\",\n",
    "    \"alimentacion_wattage_binned\",\n",
    "    \n",
    "]\n",
    "print(f\"User-specified features for LAPTOP model training: {user_feature_columns_laptop}\")\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # !!! IMPORTANT: Verify and correct this path for your desktop PC data !!!\n",
    "    df_laptop = pd.read_csv('C:/Users/Valentina G/Documents/GitHub/PcPartPicker3000/assignment/df_engineered_laptop.csv') # Or your actual desktop data file\n",
    "    print(\"Desktop PC data loaded successfully.\")\n",
    "    # Load other dataframes if they are used elsewhere in your notebook, otherwise they can be removed.\n",
    "    # df = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered.csv')\n",
    "    # df_laptop = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_laptop.csv')\n",
    "    # df_partial_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_partial_pc.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading desktop PC data: {e}\")\n",
    "    print(\"Please ensure the file path for df_laptop is correct.\")\n",
    "    raise\n",
    "\n",
    "# --- 2. INITIAL DATA CLEANING (Example from your snippets) ---\n",
    "# Adjust if these columns are not relevant or if other cleaning is needed for df_laptop\n",
    "columns_to_drop = ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
    "existing_cols_desktop = [col for col in columns_to_drop if col in df_laptop.columns]\n",
    "if existing_cols_desktop:\n",
    "    df_laptop = df_laptop.drop(columns=existing_cols_desktop)\n",
    "    print(f\"Dropped columns from df_laptop (if they existed): {existing_cols_desktop}\")\n",
    "\n",
    "\n",
    "# 2. Check for existence of these columns in df_laptop and prepare X and y\n",
    "# Ensure df_laptop is loaded and preprocessed from previous cells\n",
    "if 'df_laptop' not in globals() or df_laptop.empty:\n",
    "    print(\"Error: df_laptop is not loaded or is empty. Please run previous cells to load data.\")\n",
    "    # Or raise an error: raise ValueError(\"df_laptop is not loaded or is empty.\")\n",
    "else:\n",
    "    available_features_laptop = [col for col in user_feature_columns_laptop if col in df_laptop.columns]\n",
    "    missing_features_laptop = [col for col in user_feature_columns_laptop if col not in df_laptop.columns]\n",
    "\n",
    "    if missing_features_laptop:\n",
    "        print(f\"\\nWarning: The following specified features were NOT found in df_laptop and will be excluded: {missing_features_laptop}\")\n",
    "    \n",
    "    if not available_features_laptop:\n",
    "        print(\"\\nError: None of the specified features were found in df_laptop. Cannot train the model.\")\n",
    "    elif 'precio_mean' not in df_laptop.columns:\n",
    "        print(\"\\nError: Target column 'precio_mean' not found in df_laptop. Cannot train the model.\")\n",
    "    else:\n",
    "        print(f\"\\nUsing the following available features for LAPTOP training: {available_features_laptop}\")\n",
    "        \n",
    "        X_specified_laptop = df_laptop[available_features_laptop]\n",
    "        y_specified_laptop = df_laptop['precio_mean']\n",
    "\n",
    "        # 3. Define categorical and numerical features from the available_features_laptop list\n",
    "        categorical_features_laptop = X_specified_laptop.select_dtypes(include=['object', 'category']).columns\n",
    "        numerical_features_laptop = X_specified_laptop.select_dtypes(include=['number']).columns\n",
    "\n",
    "        print(f\"Categorical features for this LAPTOP model: {list(categorical_features_laptop)}\")\n",
    "        print(f\"Numerical features for this LAPTOP model: {list(numerical_features_laptop)}\")\n",
    "\n",
    "        # 4. Define preprocessor\n",
    "        numerical_transformer_laptop = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer_laptop = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "\n",
    "        preprocessor_laptop = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer_laptop, numerical_features_laptop),\n",
    "                ('cat', categorical_transformer_laptop, categorical_features_laptop)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        # 5. Split data\n",
    "        X_train_laptop, X_test_laptop, y_train_laptop, y_test_laptop = train_test_split(\n",
    "            X_specified_laptop, y_specified_laptop, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # 6. Define the LightGBM model and the hyperparameter grid\n",
    "        # Using the expanded grid for overnight tuning\n",
    "        param_grid_laptop = {\n",
    "            'regressor__n_estimators': [500],\n",
    "            'regressor__learning_rate': [0.05],\n",
    "            'regressor__num_leaves': [70],\n",
    "            'regressor__max_depth': [10],\n",
    "            'regressor__min_child_samples': [20],\n",
    "            'regressor__colsample_bytree': [0.8],\n",
    "            'regressor__subsample': [0.8],\n",
    "            'regressor__reg_alpha': [0.001],\n",
    "            'regressor__reg_lambda': [0.5]\n",
    "        }\n",
    "\n",
    "        pipeline_laptop = Pipeline(steps=[('preprocessor', preprocessor_laptop),\n",
    "                                          ('regressor', lgb.LGBMRegressor(random_state=42, verbose=-1))])\n",
    "\n",
    "        grid_search_laptop = GridSearchCV(pipeline_laptop, param_grid_laptop, \n",
    "                                          cv=5, scoring='neg_mean_squared_error', \n",
    "                                          verbose=2, n_jobs=-1) # verbose=2 for more output\n",
    "\n",
    "        print(\"\\nFitting GridSearchCV with user-specified features for LAPTOPS...\")\n",
    "        grid_search_laptop.fit(X_train_laptop, y_train_laptop)\n",
    "        print(\"\\nGridSearchCV fitting process complete. Extracting best parameters...\") # <-- New print statement\n",
    "\n",
    "        print(\"\\nBest parameters found by GridSearchCV (User-Specified Features for LAPTOPS):\")\n",
    "        print(grid_search_laptop.best_params_)\n",
    "\n",
    "        best_model_laptop = grid_search_laptop.best_estimator_\n",
    "        print(\"\\nLAPTOP model training complete using best estimator from GridSearchCV (User-Specified Features).\")\n",
    "\n",
    "        # 7. Evaluate on the Test Set\n",
    "        y_pred_test_laptop = best_model_laptop.predict(X_test_laptop)\n",
    "        test_mse_laptop = mean_squared_error(y_test_laptop, y_pred_test_laptop)\n",
    "        test_rmse_laptop = np.sqrt(test_mse_laptop)\n",
    "        test_r2_laptop = r2_score(y_test_laptop, y_pred_test_laptop)\n",
    "\n",
    "        print(\"\\nMetrics on the LAPTOP TEST Set (User-Specified Features):\")\n",
    "        print(f\"Mean Squared Error (MSE): {test_mse_laptop:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {test_rmse_laptop:.4f}\")\n",
    "        print(f\"R-squared: {test_r2_laptop:.4f}\")\n",
    "\n",
    "        # 8. Evaluate on the Training Set\n",
    "        y_pred_train_laptop = best_model_laptop.predict(X_train_laptop)\n",
    "        train_mse_laptop = mean_squared_error(y_train_laptop, y_pred_train_laptop)\n",
    "        train_rmse_laptop = np.sqrt(train_mse_laptop)\n",
    "        train_r2_laptop = r2_score(y_train_laptop, y_pred_train_laptop)\n",
    "\n",
    "        print(\"\\nMetrics on the LAPTOP TRAINING Set (User-Specified Features):\")\n",
    "        print(f\"Mean Squared Error (MSE): {train_mse_laptop:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {train_rmse_laptop:.4f}\")\n",
    "        print(f\"R-squared: {train_r2_laptop:.4f}\")\n",
    "\n",
    "print(\"\\n--- LAPTOP Model Training with User-Specified Features Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Starting Model Training for LAPTOPS with User-Specified Features ---\n",
    "User-specified features for LAPTOP model training: ['ram_memoria_gb', 'ram_frecuencia_de_la_memoria', 'ram_tipo', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador', 'procesador_frecuencia_turbo_max_ghz', 'grafica_tarjeta', 'pantalla_resolucion_pulgadas', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi']\n",
    "Loading data...\n",
    "Desktop PC data loaded successfully.\n",
    "Dropped columns from df_laptop (if they existed): ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
    "\n",
    "Warning: The following specified features were NOT found in df_laptop and will be excluded: ['ram_frecuencia_de_la_memoria', 'pantalla_resolucion_pulgadas']\n",
    "\n",
    "Using the following available features for LAPTOP training: ['ram_memoria_gb', 'ram_tipo', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador', 'procesador_frecuencia_turbo_max_ghz', 'grafica_tarjeta', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi']\n",
    "Categorical features for this LAPTOP model: ['ram_tipo', 'procesador', 'grafica_tarjeta', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi']\n",
    "Numerical features for this LAPTOP model: ['ram_memoria_gb', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador_frecuencia_turbo_max_ghz']\n",
    "\n",
    "Fitting GridSearchCV with user-specified features for LAPTOPS...\n",
    "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
    "\n",
    "Best parameters found by GridSearchCV (User-Specified Features for LAPTOPS):\n",
    "{'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 10, 'regressor__min_child_samples': 20, 'regressor__n_estimators': 500, 'regressor__num_leaves': 70, 'regressor__reg_alpha': 0.001, 'regressor__reg_lambda': 0.5, 'regressor__subsample': 0.8}\n",
    "\n",
    "LAPTOP model training complete using best estimator from GridSearchCV (User-Specified Features).\n",
    "\n",
    "Metrics on the LAPTOP TEST Set (User-Specified Features):\n",
    "Mean Squared Error (MSE): 578575.7598\n",
    "Root Mean Squared Error (RMSE): 760.6417\n",
    "R-squared: 0.6599\n",
    "\n",
    "Metrics on the LAPTOP TRAINING Set (User-Specified Features):\n",
    "Mean Squared Error (MSE): 468260.8446\n",
    "Root Mean Squared Error (RMSE): 684.2959\n",
    "R-squared: 0.6932\n",
    "\n",
    "--- LAPTOP Model Training with User-Specified Features Complete ---\n",
    "C:\\Users\\Valentina G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
    "  warnings.warn(\n",
    "C:\\Users\\Valentina G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Error loading desktop PC data: [Errno 2] No such file or directory: '/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_desktop_pc.csv'\n",
      "Please ensure the file path for df_desktop_pc is correct.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_desktop_pc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# !!! IMPORTANT: Verify and correct this path for your desktop PC data !!!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     df_desktop_pc = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_desktop_pc.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Or your actual desktop data file\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDesktop PC data loaded successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Load other dataframes if they are used elsewhere in your notebook, otherwise they can be removed.\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# df = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered.csv')\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# df_laptop = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_laptop.csv')\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# df_partial_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_partial_pc.csv')\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_desktop_pc.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb # <--- Added XGBoost import\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.ensemble import GradientBoostingRegressor # Kept if you might switch back\n",
    "from sklearn.metrics import mean_squared_error, r2_score # make_scorer is available if needed\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb # Kept for the feature selection part, can be removed if FS model also changes\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # !!! IMPORTANT: Verify and correct this path for your desktop PC data !!!\n",
    "    df_desktop_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_desktop_pc.csv') # Or your actual desktop data file\n",
    "    print(\"Desktop PC data loaded successfully.\")\n",
    "    # Load other dataframes if they are used elsewhere in your notebook, otherwise they can be removed.\n",
    "    # df = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered.csv')\n",
    "    # df_laptop = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_laptop.csv')\n",
    "    # df_partial_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_partial_pc.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading desktop PC data: {e}\")\n",
    "    print(\"Please ensure the file path for df_desktop_pc is correct.\")\n",
    "    raise\n",
    "\n",
    "# --- 2. INITIAL DATA CLEANING (Example from your snippets) ---\n",
    "# Adjust if these columns are not relevant or if other cleaning is needed for df_desktop_pc\n",
    "columns_to_drop = ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
    "existing_cols_desktop = [col for col in columns_to_drop if col in df_desktop_pc.columns]\n",
    "if existing_cols_desktop:\n",
    "    df_desktop_pc = df_desktop_pc.drop(columns=existing_cols_desktop)\n",
    "    print(f\"Dropped columns from df_desktop_pc (if they existed): {existing_cols_desktop}\")\n",
    "\n",
    "\n",
    "# --- 3. SANITY CHECKS FOR df_desktop_pc ---\n",
    "if 'precio_mean' not in df_desktop_pc.columns:\n",
    "    raise ValueError(\"Target column 'precio_mean' not found in df_desktop_pc after initial processing.\")\n",
    "if df_desktop_pc.empty:\n",
    "    raise ValueError(\"The DataFrame df_desktop_pc is empty. Cannot train the model.\")\n",
    "\n",
    "# --- 4. DEFINE INITIAL FEATURES FOR SELECTION ---\n",
    "initial_columns_to_consider = [col for col in df_desktop_pc.columns if col != 'precio_mean']\n",
    "if not initial_columns_to_consider:\n",
    "    raise ValueError(\"No feature columns found in df_desktop_pc after excluding 'precio_mean'.\")\n",
    "print(f\"Initial columns for feature selection consideration: {initial_columns_to_consider}\")\n",
    "\n",
    "# --- 5. FEATURE SELECTION BLOCK (using LightGBM for importances, can be changed) ---\n",
    "# This block remains the same, using LightGBM to determine feature importances.\n",
    "# If you want to use XGBoost for feature selection as well, this part would need modification.\n",
    "print(\"\\n--- Starting Feature Selection Process (using LightGBM for importances) ---\")\n",
    "\n",
    "X_initial = df_desktop_pc[initial_columns_to_consider]\n",
    "y_initial = df_desktop_pc['precio_mean']\n",
    "\n",
    "categorical_features_initial = X_initial.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features_initial = X_initial.select_dtypes(include=['number']).columns\n",
    "\n",
    "print(f\"Initial categorical features for FS: {list(categorical_features_initial)}\")\n",
    "print(f\"Initial numerical features for FS: {list(numerical_features_initial)}\")\n",
    "\n",
    "numerical_transformer_fs = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer_fs = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor_fs = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer_fs, numerical_features_initial),\n",
    "        ('cat', categorical_transformer_fs, categorical_features_initial)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "X_train_fs, _, y_train_fs, _ = train_test_split(X_initial, y_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "lgbm_fs = lgb.LGBMRegressor(random_state=42, verbose=-1) # Using LightGBM for feature selection\n",
    "pipeline_fs = Pipeline(steps=[('preprocessor', preprocessor_fs), ('regressor', lgbm_fs)])\n",
    "\n",
    "print(\"Fitting pipeline for feature selection (LightGBM)...\")\n",
    "pipeline_fs.fit(X_train_fs, y_train_fs)\n",
    "print(\"Feature selection pipeline fitting complete.\")\n",
    "\n",
    "importances_fs = pipeline_fs.named_steps['regressor'].feature_importances_\n",
    "transformed_feature_names_fs = pipeline_fs.named_steps['preprocessor'].get_feature_names_out()\n",
    "original_feature_importances = {col: 0.0 for col in initial_columns_to_consider}\n",
    "\n",
    "for i, full_transformed_name in enumerate(transformed_feature_names_fs):\n",
    "    importance_value = importances_fs[i]\n",
    "    name_parts = full_transformed_name.split('__', 1)\n",
    "    if len(name_parts) < 2: continue\n",
    "    transformer_prefix, internal_name = name_parts\n",
    "    if transformer_prefix == 'num' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "    elif transformer_prefix == 'cat':\n",
    "        for original_cat_col in categorical_features_initial:\n",
    "            if internal_name == original_cat_col or internal_name.startswith(original_cat_col + \"_\"):\n",
    "                original_feature_importances[original_cat_col] += importance_value\n",
    "                break\n",
    "    elif transformer_prefix == 'remainder' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "\n",
    "sorted_original_features = sorted(original_feature_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"\\nFeature importances (aggregated from initial set using LightGBM):\")\n",
    "for feature, score in sorted_original_features: print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "columns_to_keep = [feature for feature, score in sorted_original_features if score > 0]\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nWarning: No features found with importance score > 0 from LightGBM FS.\")\n",
    "else:\n",
    "    print(f\"\\nSelected {len(columns_to_keep)} features (importance > 0 from LightGBM FS) for main model: {columns_to_keep}\")\n",
    "print(\"--- Feature Selection Process Complete ---\")\n",
    "\n",
    "\n",
    "# --- 6. MAIN MODEL TRAINING AND EVALUATION with XGBoost ---\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nHalting script: No features selected for the main model.\")\n",
    "else:\n",
    "    print(\"\\n--- Starting Main Model Training and Evaluation with XGBoost ---\")\n",
    "    X = df_desktop_pc[columns_to_keep]\n",
    "    y = df_desktop_pc['precio_mean']\n",
    "\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "    print(f\"Categorical features for XGBoost model: {list(categorical_features)}\")\n",
    "    print(f\"Numerical features for XGBoost model: {list(numerical_features)}\")\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # XGBoost Parameter Grid for GridSearchCV\n",
    "    # Note: XGBoost can handle categorical features with its 'enable_categorical=True' experimental feature,\n",
    "    # but it's often more robust to one-hot encode them as we are doing.\n",
    "    param_grid_xgb = {\n",
    "        'regressor__n_estimators': [100, 200, 300],\n",
    "        # 'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "        # 'regressor__max_depth': [3, 5, 7], # Typical values for XGBoost\n",
    "        # 'regressor__subsample': [0.7, 0.8, 1.0],\n",
    "        # 'regressor__colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        # 'regressor__gamma': [0, 0.1, 0.2], # Minimum loss reduction required to make a further partition\n",
    "        # 'regressor__reg_alpha': [0, 0.01, 0.1], # L1 regularization\n",
    "        # 'regressor__reg_lambda': [1, 0.1, 0.01]  # L2 regularization (XGBoost default is 1)\n",
    "    }\n",
    "\n",
    "    # Create the pipeline with XGBoost Regressor\n",
    "    # XGBoost may issue warnings about unsupported 'verbose' if passed from LGBM feature selector;\n",
    "    # we define a new regressor instance here.\n",
    "    pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', xgb.XGBRegressor(random_state=42, objective='reg:squarederror'))])\n",
    "                                   # objective='reg:squarederror' suppresses a warning in newer XGBoost versions.\n",
    "\n",
    "    grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "    print(\"Fitting GridSearchCV with XGBoost...\")\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters found by GridSearchCV for XGBoost:\")\n",
    "    print(grid_search_xgb.best_params_)\n",
    "\n",
    "    best_model_xgb = grid_search_xgb.best_estimator_\n",
    "    print(\"\\nXGBoost model pipeline fitting complete using best estimator from GridSearchCV.\")\n",
    "\n",
    "    # Evaluate on the Test Set\n",
    "    y_pred_test_xgb = best_model_xgb.predict(X_test)\n",
    "    test_mse_xgb = mean_squared_error(y_test, y_pred_test_xgb)\n",
    "    test_rmse_xgb = np.sqrt(test_mse_xgb)\n",
    "    test_r2_xgb = r2_score(y_test, y_pred_test_xgb)\n",
    "\n",
    "    print(\"\\nMetrics on the TEST Set (using best XGBoost model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {test_mse_xgb:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {test_rmse_xgb:.4f}\")\n",
    "    print(f\"R-squared: {test_r2_xgb:.4f}\")\n",
    "\n",
    "    # Evaluate on the Training Set\n",
    "    y_pred_train_xgb = best_model_xgb.predict(X_train)\n",
    "    train_mse_xgb = mean_squared_error(y_train, y_pred_train_xgb)\n",
    "    train_rmse_xgb = np.sqrt(train_mse_xgb)\n",
    "    train_r2_xgb = r2_score(y_train, y_pred_train_xgb)\n",
    "\n",
    "    print(\"\\nMetrics on the TRAINING Set (using best XGBoost model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {train_mse_xgb:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {train_rmse_xgb:.4f}\")\n",
    "    print(f\"R-squared: {train_r2_xgb:.4f}\")\n",
    "\n",
    "print(\"\\n--- Script Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor # <--- Added MLPRegressor import\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb # Kept for the feature selection part\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # !!! IMPORTANT: Verify and correct this path for your desktop PC data !!!\n",
    "    df_desktop_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_desktop_pc.csv') # Or your actual desktop data file\n",
    "    print(\"Desktop PC data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading desktop PC data: {e}\")\n",
    "    print(\"Please ensure the file path for df_desktop_pc is correct.\")\n",
    "    raise\n",
    "\n",
    "# --- 2. INITIAL DATA CLEANING ---\n",
    "columns_to_drop = ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
    "existing_cols_desktop = [col for col in columns_to_drop if col in df_desktop_pc.columns]\n",
    "if existing_cols_desktop:\n",
    "    df_desktop_pc = df_desktop_pc.drop(columns=existing_cols_desktop)\n",
    "    print(f\"Dropped columns from df_desktop_pc (if they existed): {existing_cols_desktop}\")\n",
    "\n",
    "# --- 3. SANITY CHECKS FOR df_desktop_pc ---\n",
    "if 'precio_mean' not in df_desktop_pc.columns:\n",
    "    raise ValueError(\"Target column 'precio_mean' not found in df_desktop_pc.\")\n",
    "if df_desktop_pc.empty:\n",
    "    raise ValueError(\"The DataFrame df_desktop_pc is empty.\")\n",
    "\n",
    "# --- 4. DEFINE INITIAL FEATURES FOR SELECTION ---\n",
    "initial_columns_to_consider = [col for col in df_desktop_pc.columns if col != 'precio_mean']\n",
    "if not initial_columns_to_consider:\n",
    "    raise ValueError(\"No feature columns found in df_desktop_pc after excluding 'precio_mean'.\")\n",
    "print(f\"Initial columns for feature selection: {initial_columns_to_consider}\")\n",
    "\n",
    "# --- 5. FEATURE SELECTION BLOCK (using LightGBM for importances) ---\n",
    "print(\"\\n--- Starting Feature Selection Process (using LightGBM for importances) ---\")\n",
    "X_initial = df_desktop_pc[initial_columns_to_consider]\n",
    "y_initial = df_desktop_pc['precio_mean']\n",
    "\n",
    "categorical_features_initial = X_initial.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features_initial = X_initial.select_dtypes(include=['number']).columns\n",
    "\n",
    "numerical_transformer_fs = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer_fs = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_fs = ColumnTransformer(transformers=[('num', numerical_transformer_fs, numerical_features_initial), ('cat', categorical_transformer_fs, categorical_features_initial)], remainder='passthrough')\n",
    "\n",
    "X_train_fs, _, y_train_fs, _ = train_test_split(X_initial, y_initial, test_size=0.2, random_state=42)\n",
    "lgbm_fs = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "pipeline_fs = Pipeline(steps=[('preprocessor', preprocessor_fs), ('regressor', lgbm_fs)])\n",
    "\n",
    "print(\"Fitting pipeline for feature selection (LightGBM)...\")\n",
    "pipeline_fs.fit(X_train_fs, y_train_fs)\n",
    "print(\"Feature selection pipeline fitting complete.\")\n",
    "\n",
    "importances_fs = pipeline_fs.named_steps['regressor'].feature_importances_\n",
    "transformed_feature_names_fs = pipeline_fs.named_steps['preprocessor'].get_feature_names_out()\n",
    "original_feature_importances = {col: 0.0 for col in initial_columns_to_consider}\n",
    "\n",
    "for i, full_transformed_name in enumerate(transformed_feature_names_fs):\n",
    "    importance_value = importances_fs[i]\n",
    "    name_parts = full_transformed_name.split('__', 1)\n",
    "    if len(name_parts) < 2: continue\n",
    "    transformer_prefix, internal_name = name_parts\n",
    "    if transformer_prefix == 'num' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "    elif transformer_prefix == 'cat':\n",
    "        for original_cat_col in categorical_features_initial:\n",
    "            if internal_name == original_cat_col or internal_name.startswith(original_cat_col + \"_\"):\n",
    "                original_feature_importances[original_cat_col] += importance_value\n",
    "                break\n",
    "    elif transformer_prefix == 'remainder' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "\n",
    "sorted_original_features = sorted(original_feature_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"\\nFeature importances (aggregated from LightGBM FS):\")\n",
    "for feature, score in sorted_original_features: print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "columns_to_keep = [feature for feature, score in sorted_original_features if score > 0]\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nWarning: No features found with importance > 0 from LightGBM FS.\")\n",
    "else:\n",
    "    print(f\"\\nSelected {len(columns_to_keep)} features (importance > 0 from LightGBM FS) for main model: {columns_to_keep}\")\n",
    "print(\"--- Feature Selection Process Complete ---\")\n",
    "\n",
    "# --- 6. MAIN MODEL TRAINING AND EVALUATION with MLPRegressor ---\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nHalting script: No features selected for the main model.\")\n",
    "else:\n",
    "    print(\"\\n--- Starting Main Model Training and Evaluation with MLPRegressor ---\")\n",
    "    X = df_desktop_pc[columns_to_keep]\n",
    "    y = df_desktop_pc['precio_mean']\n",
    "\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "    print(f\"Categorical features for MLP model: {list(categorical_features)}\")\n",
    "    print(f\"Numerical features for MLP model: {list(numerical_features)}\")\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]) # Scaling is crucial for NNs\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_features), ('cat', categorical_transformer, categorical_features)], remainder='passthrough')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # MLPRegressor Parameter Grid for GridSearchCV\n",
    "    param_grid_mlp = {\n",
    "        'regressor__hidden_layer_sizes': [(50,), (100,), (50,25)], # One or two hidden layers\n",
    "        # 'regressor__activation': ['relu', 'tanh'],\n",
    "        # 'regressor__solver': ['adam'], # Adam is often a good default\n",
    "        'regressor__alpha': [0.0001, 0.001, 0.01], # L2 regularization\n",
    "        # 'regressor__learning_rate_init': [0.001, 0.01],\n",
    "        # 'regressor__max_iter': [300, 500] # Allow more iterations for convergence\n",
    "        'regressor__early_stopping': [True], # Can help prevent overfitting and speed up grid search\n",
    "        # 'regressor__n_iter_no_change': [10]   # Used with early_stopping\n",
    "    }\n",
    "\n",
    "    pipeline_mlp = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', MLPRegressor(random_state=42))])\n",
    "\n",
    "    grid_search_mlp = GridSearchCV(pipeline_mlp, param_grid_mlp, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "    print(\"Fitting GridSearchCV with MLPRegressor...\")\n",
    "    grid_search_mlp.fit(X_train, y_train) # NNs can take longer to train, especially with GridSearchCV\n",
    "\n",
    "    print(\"\\nBest parameters found by GridSearchCV for MLPRegressor:\")\n",
    "    print(grid_search_mlp.best_params_)\n",
    "\n",
    "    best_model_mlp = grid_search_mlp.best_estimator_\n",
    "    print(\"\\nMLPRegressor model pipeline fitting complete using best estimator.\")\n",
    "\n",
    "    # Evaluate on the Test Set\n",
    "    y_pred_test_mlp = best_model_mlp.predict(X_test)\n",
    "    test_mse_mlp = mean_squared_error(y_test, y_pred_test_mlp)\n",
    "    test_rmse_mlp = np.sqrt(test_mse_mlp)\n",
    "    test_r2_mlp = r2_score(y_test, y_pred_test_mlp)\n",
    "\n",
    "    print(\"\\nMetrics on the TEST Set (using best MLPRegressor model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {test_mse_mlp:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {test_rmse_mlp:.4f}\")\n",
    "    print(f\"R-squared: {test_r2_mlp:.4f}\")\n",
    "\n",
    "    # Evaluate on the Training Set\n",
    "    y_pred_train_mlp = best_model_mlp.predict(X_train)\n",
    "    train_mse_mlp = mean_squared_error(y_train, y_pred_train_mlp)\n",
    "    train_rmse_mlp = np.sqrt(train_mse_mlp)\n",
    "    train_r2_mlp = r2_score(y_train, y_pred_train_mlp)\n",
    "\n",
    "    print(\"\\nMetrics on the TRAINING Set (using best MLPRegressor model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {train_mse_mlp:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {train_rmse_mlp:.4f}\")\n",
    "    print(f\"R-squared: {train_r2_mlp:.4f}\")\n",
    "\n",
    "print(\"\\n--- Script Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 230 minutes of running, this was the result of the cross validation:\n",
    "\n",
    "GridSearchCV Complete.\n",
    "Best parameters found: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 10, 'regressor__min_child_samples': 20, 'regressor__n_estimators': 500, 'regressor__num_leaves': 70, 'regressor__subsample': 0.8}\n",
    "\n",
    "Best cross-validation score (negative MSE): -375894.4673686046\n",
    "\n",
    "Best cross-validation RMSE: 613.1023302586646\n",
    "\n",
    "Metrics on the Test Set using the Best Model from GridSearchCV:\n",
    "\n",
    "Mean Squared Error (MSE): 347437.5443584199\n",
    "\n",
    "Root Mean Squared Error (RMSE): 589.4383295633394\n",
    "\n",
    "R-squared: 0.7957941987120332\n",
    "Metrics on the Training Set using the Best Model from GridSearchCV:\n",
    "\n",
    "Mean Squared Error (MSE): 73745.4619638589\n",
    "\n",
    "Root Mean Squared Error (RMSE): 271.5611569496987\n",
    "\n",
    "R-squared: 0.9516823528897075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
