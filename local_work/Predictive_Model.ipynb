{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating an analytics type application where the user can explore the dataset provided with an interactive UI. One page in our application will allow the user to choose different features of a PC, Laptop, Or Partially built PC in order to provide the predicted price based on certain specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.0-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.5/11.0 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 27.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "Successfully installed pandas-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.1.3)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Downloading numpy-2.3.1-cp313-cp313-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.8/12.7 MB 32.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.7 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 29.7 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed numpy-2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.14.1 requires numpy<2.3,>=1.23.5, but you have numpy 2.3.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.5.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Downloading numpy-2.2.6-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 5.8/10.7 MB 32.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 29.1 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.6-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 29.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 33.0 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, scikit-learn\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.3.1\n",
      "\n",
      "    Uninstalling numpy-2.3.1:\n",
      "\n",
      "      Successfully uninstalled numpy-2.3.1\n",
      "\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "  Attempting uninstall: scikit-learn\n",
      "   ---------------------------------------- 0/2 [numpy]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "    Found existing installation: scikit-learn 1.5.2\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "    Uninstalling scikit-learn-1.5.2:\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "      Successfully uninstalled scikit-learn-1.5.2\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   ---------------------------------------- 2/2 [scikit-learn]\n",
      "\n",
      "Successfully installed numpy-2.2.6 scikit-learn-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\~klearn'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from lightgbm) (1.14.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 22.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from xgboost) (1.14.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 4.5/150.0 MB 33.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 12.1/150.0 MB 31.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 19.4/150.0 MB 32.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 26.2/150.0 MB 32.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 33.8/150.0 MB 33.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 41.2/150.0 MB 33.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 48.2/150.0 MB 33.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 55.6/150.0 MB 33.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 62.9/150.0 MB 33.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 70.0/150.0 MB 33.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 77.1/150.0 MB 33.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 84.1/150.0 MB 33.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 91.0/150.0 MB 33.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 98.6/150.0 MB 34.0 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 105.4/150.0 MB 34.0 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 112.7/150.0 MB 34.0 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 119.8/150.0 MB 34.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 127.1/150.0 MB 34.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 134.2/150.0 MB 34.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 141.6/150.0 MB 34.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  148.4/150.0 MB 34.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 34.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 32.0 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 5.5/8.1 MB 30.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 29.3 MB/s eta 0:00:00\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.1\n",
      "    Uninstalling matplotlib-3.10.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'c:\\\\users\\\\mmeri\\\\appdata\\\\local\\\\programs\\\\python\\\\python313\\\\lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp313-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
      "  Using cached matplotlib-3.10.3-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mmeri\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached matplotlib-3.10.3-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "Installing collected packages: matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'c:\\\\Users\\\\mmeri\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp313-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Ensure the executable path is quoted to handle spaces\n",
    "!\"{sys.executable}\" -m pip install --upgrade pandas\n",
    "!\"{sys.executable}\" -m pip install --upgrade numpy\n",
    "!\"{sys.executable}\" -m pip install --upgrade scikit-learn\n",
    "!\"{sys.executable}\" -m pip install --upgrade lightgbm\n",
    "!\"{sys.executable}\" -m pip install --upgrade xgboost\n",
    "!\"{sys.executable}\" -m pip install --upgrade matplotlib\n",
    "!\"{sys.executable}\" -m pip install --upgrade seaborn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Starting Model Training for LAPTOPS with User-Specified Features ---\n",
    "User-specified features for LAPTOP model training: ['ram_memoria_gb', 'ram_frecuencia_de_la_memoria', 'ram_tipo', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador', 'procesador_frecuencia_turbo_max_ghz', 'grafica_tarjeta', 'pantalla_resolucion_pulgadas', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi']\n",
    "Loading data...\n",
    "Desktop PC data loaded successfully.\n",
    "Dropped columns from df_laptop (if they existed): ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
    "\n",
    "Warning: The following specified features were NOT found in df_laptop and will be excluded: ['ram_frecuencia_de_la_memoria', 'pantalla_resolucion_pulgadas']\n",
    "\n",
    "Using the following available features for LAPTOP training: ['ram_memoria_gb', 'ram_tipo', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador', 'procesador_frecuencia_turbo_max_ghz', 'grafica_tarjeta', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi']\n",
    "Categorical features for this LAPTOP model: ['ram_tipo', 'procesador', 'grafica_tarjeta', 'sistema_operativo_sistema_operativo', 'comunicaciones_estandar_wifi']\n",
    "Numerical features for this LAPTOP model: ['ram_memoria_gb', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador_frecuencia_turbo_max_ghz']\n",
    "\n",
    "Fitting GridSearchCV with user-specified features for LAPTOPS...\n",
    "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
    "\n",
    "Best parameters found by GridSearchCV (User-Specified Features for LAPTOPS):\n",
    "{'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 10, 'regressor__min_child_samples': 20, 'regressor__n_estimators': 500, 'regressor__num_leaves': 70, 'regressor__reg_alpha': 0.001, 'regressor__reg_lambda': 0.5, 'regressor__subsample': 0.8}\n",
    "\n",
    "LAPTOP model training complete using best estimator from GridSearchCV (User-Specified Features).\n",
    "\n",
    "Metrics on the LAPTOP TEST Set (User-Specified Features):\n",
    "Mean Squared Error (MSE): 578575.7598\n",
    "Root Mean Squared Error (RMSE): 760.6417\n",
    "R-squared: 0.6599\n",
    "\n",
    "Metrics on the LAPTOP TRAINING Set (User-Specified Features):\n",
    "Mean Squared Error (MSE): 468260.8446\n",
    "Root Mean Squared Error (RMSE): 684.2959\n",
    "R-squared: 0.6932\n",
    "\n",
    "--- LAPTOP Model Training with User-Specified Features Complete ---\n",
    "C:\\Users\\Valentina G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
    "  warnings.warn(\n",
    "C:\\Users\\Valentina G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
    "  warnings.warn("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.backends.registry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_evaluate_lgbm_model\u001b[39m(\n\u001b[0;32m     13\u001b[0m     input_df: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[0;32m     14\u001b[0m     target_col_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     random_seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     22\u001b[0m ):\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    Trains and evaluates a LightGBM regression model with optional feature specification\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    and optional log transformation of the target.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m        sklearn.pipeline.Pipeline: The trained model pipeline, or None if training fails.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\__init__.py:161\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcsetup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\rcsetup.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n",
      "File \u001b[1;32mc:\\Users\\mmeri\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\matplotlib\\backends\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# NOTE: plt.switch_backend() (called at import time) will add a \"backend\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# attribute here for backcompat.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m _QT_FORCE_QT5_BINDING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib.backends.registry'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_evaluate_lgbm_model(\n",
    "    input_df: pd.DataFrame,\n",
    "    target_col_name: str,\n",
    "    model_name_suffix: str = \"Model\",\n",
    "    apply_log_transform_to_target: bool = True, # New parameter\n",
    "    user_feature_cols: list = None,\n",
    "    cols_to_always_drop: list = None,\n",
    "    lgbm_hyperparams: dict = None,\n",
    "    test_split_size: float = 0.2,\n",
    "    random_seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a LightGBM regression model with optional feature specification\n",
    "    and optional log transformation of the target.\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): The input DataFrame containing features and target.\n",
    "        target_col_name (str): The name of the target variable column.\n",
    "        model_name_suffix (str): A suffix for print statements and plot titles (e.g., \"LAPTOP\").\n",
    "        apply_log_transform_to_target (bool): If True, applies log1p to target and expm1 to predictions.\n",
    "                                              Defaults to True.\n",
    "        user_feature_cols (list, optional): A list of feature columns to use. \n",
    "                                            If None or empty, all columns (except target and \n",
    "                                            cols_to_always_drop) will be used. Defaults to None.\n",
    "        cols_to_always_drop (list, optional): List of columns to always drop before feature selection.\n",
    "                                              Defaults to ['titulo', 'precio_min', 'precio_max', 'tipo'].\n",
    "        lgbm_hyperparams (dict, optional): Hyperparameters for the LGBMRegressor. \n",
    "                                           Defaults to a predefined set if None.\n",
    "        test_split_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        sklearn.pipeline.Pipeline: The trained model pipeline, or None if training fails.\n",
    "    \"\"\"\n",
    "\n",
    "    log_transform_status = \"with Log-Transformed Target\" if apply_log_transform_to_target else \"with Original Target Scale\"\n",
    "    print(f\"\\n--- Starting Model Training for {model_name_suffix.upper()} ({log_transform_status}) ---\")\n",
    "\n",
    "    df = input_df.copy() # Work on a copy\n",
    "\n",
    "    # --- 1. INITIAL DATA CLEANING ---\n",
    "    if cols_to_always_drop is None:\n",
    "        cols_to_always_drop = ['titulo', 'precio_min', 'precio_max', 'tipo'] \n",
    "    \n",
    "    existing_cols_to_drop = [col for col in cols_to_always_drop if col in df.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        df = df.drop(columns=existing_cols_to_drop)\n",
    "        print(f\"Dropped specified columns from input_df (if they existed): {existing_cols_to_drop}\")\n",
    "\n",
    "    # --- 2. TARGET AND FEATURE PREPARATION ---\n",
    "    if target_col_name not in df.columns:\n",
    "        print(f\"\\nError: Target column '{target_col_name}' not found in the DataFrame. Cannot train model.\")\n",
    "        return None\n",
    "\n",
    "    y_original_prices = df[target_col_name]\n",
    "    y_for_model_training = y_original_prices # Default to original scale\n",
    "\n",
    "    if apply_log_transform_to_target:\n",
    "        y_for_model_training = np.log1p(y_original_prices)\n",
    "        print(f\"Applied np.log1p to the target variable '{target_col_name}'.\")\n",
    "    else:\n",
    "        print(f\"Using target variable '{target_col_name}' on its original scale for training.\")\n",
    "\n",
    "\n",
    "    selected_features_for_model = []\n",
    "    if user_feature_cols and len(user_feature_cols) > 0:\n",
    "        print(f\"\\nUsing user-specified features for {model_name_suffix} model training.\")\n",
    "        available_features = [col for col in user_feature_cols if col in df.columns]\n",
    "        missing_features = [col for col in user_feature_cols if col not in df.columns]\n",
    "\n",
    "        if missing_features:\n",
    "            print(f\"Warning: The following specified features were NOT found and will be excluded: {missing_features}\")\n",
    "        \n",
    "        if not available_features:\n",
    "            print(\"\\nError: None of the user-specified features were found in the DataFrame. Cannot train model.\")\n",
    "            return None\n",
    "        selected_features_for_model = available_features\n",
    "    else:\n",
    "        print(f\"\\nUsing ALL available features (excluding target and always_drop_cols) for {model_name_suffix} model training.\")\n",
    "        selected_features_for_model = [col for col in df.columns if col != target_col_name]\n",
    "        if not selected_features_for_model:\n",
    "            print(\"\\nError: No features available after excluding target and always_drop_cols. Cannot train model.\")\n",
    "            return None\n",
    "            \n",
    "    print(f\"Final features selected for {model_name_suffix} training: {len(selected_features_for_model)} features.\")\n",
    "\n",
    "\n",
    "    X = df[selected_features_for_model]\n",
    "\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    print(f\"Identified {len(categorical_features)} categorical features.\")\n",
    "    print(f\"Identified {len(numerical_features)} numerical features.\")\n",
    "\n",
    "    # --- 3. PREPROCESSING PIPELINES ---\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough' \n",
    "    )\n",
    "    \n",
    "    # --- 4. DATA SPLITTING ---\n",
    "    indices = np.arange(X.shape[0])\n",
    "    train_indices, test_indices, _, _ = train_test_split(\n",
    "        indices, indices, test_size=test_split_size, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    \n",
    "    y_train_for_model = y_for_model_training.iloc[train_indices]\n",
    "    # y_test_for_model is used if direct model prediction evaluation is needed before inverse transform\n",
    "    # y_test_for_model = y_for_model_training.iloc[test_indices] \n",
    "\n",
    "    y_train_original = y_original_prices.iloc[train_indices] # For training set evaluation on original scale\n",
    "    y_test_original = y_original_prices.iloc[test_indices] # For test set evaluation on original scale\n",
    "\n",
    "\n",
    "    # --- 5. MODEL DEFINITION AND TRAINING ---\n",
    "    if lgbm_hyperparams is None:\n",
    "        lgbm_hyperparams = { \n",
    "            'n_estimators': 500, 'learning_rate': 0.05, 'num_leaves': 70,\n",
    "            'max_depth': 10, 'min_child_samples': 20, 'colsample_bytree': 0.8,\n",
    "            'subsample': 0.8, 'random_state': random_seed, 'verbose': -1\n",
    "        }\n",
    "    else: \n",
    "        lgbm_hyperparams.setdefault('random_state', random_seed)\n",
    "        lgbm_hyperparams.setdefault('verbose', -1)\n",
    "\n",
    "    lgbm_model = lgb.LGBMRegressor(**lgbm_hyperparams)\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('regressor', lgbm_model)])\n",
    "\n",
    "    print(f\"\\nFitting {model_name_suffix} model (training target scale: {'log' if apply_log_transform_to_target else 'original'})...\")\n",
    "    pipeline.fit(X_train, y_train_for_model) # Train on y_for_model_training\n",
    "    print(f\"{model_name_suffix} model training complete.\")\n",
    "\n",
    "    # --- 6. EVALUATION ON TEST SET ---\n",
    "    y_pred_model_scale_test = pipeline.predict(X_test) # Predictions are on the scale model was trained on\n",
    "    \n",
    "    if apply_log_transform_to_target:\n",
    "        y_pred_orig_test = np.expm1(y_pred_model_scale_test)\n",
    "    else:\n",
    "        y_pred_orig_test = y_pred_model_scale_test # Predictions are already on original scale\n",
    "    y_pred_orig_test = np.maximum(0, y_pred_orig_test) \n",
    "\n",
    "    test_mse = mean_squared_error(y_test_original, y_pred_orig_test)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_r2 = r2_score(y_test_original, y_pred_orig_test)\n",
    "\n",
    "    print(f\"\\nMetrics on the {model_name_suffix.upper()} TEST Set (Evaluated on Original Scale):\")\n",
    "    print(f\"  Mean Squared Error (MSE): {test_mse:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {test_rmse:.4f}\")\n",
    "    print(f\"  R-squared (R²): {test_r2:.4f}\")\n",
    "\n",
    "    # --- 7. EVALUATION ON TRAINING SET ---\n",
    "    y_pred_model_scale_train = pipeline.predict(X_train)\n",
    "    \n",
    "    if apply_log_transform_to_target:\n",
    "        y_pred_orig_train = np.expm1(y_pred_model_scale_train)\n",
    "    else:\n",
    "        y_pred_orig_train = y_pred_model_scale_train\n",
    "    y_pred_orig_train = np.maximum(0, y_pred_orig_train)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train_original, y_pred_orig_train)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_r2 = r2_score(y_train_original, y_pred_orig_train)\n",
    "\n",
    "    print(f\"\\nMetrics on the {model_name_suffix.upper()} TRAINING Set (Evaluated on Original Scale):\")\n",
    "    print(f\"  Mean Squared Error (MSE): {train_mse:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {train_rmse:.4f}\")\n",
    "    print(f\"  R-squared (R²): {train_r2:.4f}\")\n",
    "\n",
    "    # --- 8. SCATTER PLOT (TEST SET - ORIGINAL SCALE) ---\n",
    "    plot_title_suffix = \"(Log-Target Model)\" if apply_log_transform_to_target else \"(Original Scale Target Model)\"\n",
    "    print(f\"\\nGenerating scatter plot for {model_name_suffix} Model {plot_title_suffix}: Predicted vs Actual Prices (Original Scale)...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_original, y_pred_orig_test, alpha=0.5, edgecolors='k', s=80)\n",
    "\n",
    "    min_val = min(y_test_original.min(), y_pred_orig_test.min())\n",
    "    max_val = max(y_test_original.max(), y_pred_orig_test.max())\n",
    "    if pd.notna(min_val) and pd.notna(max_val) and np.isfinite(min_val) and np.isfinite(max_val):\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    else:\n",
    "        print(\"Warning: Could not plot diagonal line due to non-finite min/max values for scatter plot axes.\")\n",
    "\n",
    "    plt.xlabel(f\"Actual Prices (Original Scale - {model_name_suffix} Test Set)\")\n",
    "    plt.ylabel(f\"Predicted Prices (Original Scale - {model_name_suffix} Test Set)\")\n",
    "    plt.title(f\"{model_name_suffix} Model {plot_title_suffix}: Actual vs. Predicted Prices (Original Scale)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"Scatter plot displayed.\")\n",
    "    \n",
    "    print(f\"\\n--- {model_name_suffix.upper()} Model Training and Evaluation Complete ({log_transform_status}) ---\")\n",
    "    return pipeline\n",
    "\n",
    "# --- Example Usage of the Updated Generic Function ---\n",
    "\n",
    "# ... (all your existing code from Predictive_Model.ipynb down to line 222) ...\n",
    "# i.e., import statements and the train_evaluate_lgbm_model function definition\n",
    "\n",
    "# --- Example Usage of the Updated Generic Function ---\n",
    "\n",
    "# 1. Load your data first\n",
    "try:\n",
    "    df_laptop_data = pd.read_csv('df_engineered_laptop.csv') \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: df_engineered_laptop.csv not found. Cannot run laptop example.\")\n",
    "    df_laptop_data = None\n",
    "\n",
    "try:\n",
    "    df_desktop_data = pd.read_csv('df_engineered_desktop_pc.csv') # Assuming you have this\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: df_engineered_desktop_pc.csv not found. Cannot run desktop example.\") # Corrected filename\n",
    "    df_desktop_data = None\n",
    "\n",
    "# --- !!! IMPORT JOBLIB !!! ---\n",
    "import joblib\n",
    "\n",
    "# --- Feature Lists (Ensure these align with your Cloud Function's expectations) ---\n",
    "# Option 1: If your Cloud Function expects these specific lists of features\n",
    "# Define them here. These must match DESKTOP_FEATURES and LAPTOP_FEATURES in main.py\n",
    "user_laptop_features_for_joblib = [\n",
    "    'procesador', 'procesador_frecuencia_turbo_max_ghz', 'procesador_numero_nucleos',\n",
    "    'grafica_tarjeta', 'disco_duro_capacidad_de_memoria_ssd_gb', 'ram_memoria_gb',\n",
    "    'ram_tipo', 'ram_frecuencia_de_la_memoria_mhz', 'sistema_operativo_sistema_operativo',\n",
    "    'comunicaciones_version_bluetooth', 'alimentacion_vatios_hora',\n",
    "    'camara_resolucion_pixeles', 'pantalla_tecnologia', 'pantalla_resolucion_pixeles'\n",
    "]\n",
    "user_desktop_features_for_joblib = [\n",
    "    'procesador', 'procesador_frecuencia_turbo_max_ghz', 'procesador_numero_nucleos',\n",
    "    'grafica_tarjeta', 'disco_duro_capacidad_de_memoria_ssd_gb', 'ram_memoria_gb',\n",
    "    'ram_tipo', 'ram_frecuencia_de_la_memoria_mhz', 'sistema_operativo_sistema_operativo',\n",
    "    'comunicaciones_version_bluetooth', 'alimentacion_wattage_binned'\n",
    "]\n",
    "# Option 2: If your Cloud Function will receive all features and rely on the\n",
    "# model's preprocessor to select the right ones (based on how it was trained),\n",
    "# then you might pass user_feature_cols=None to train_evaluate_lgbm_model,\n",
    "# but ensure the DESKTOP_FEATURES/LAPTOP_FEATURES in main.py are still correct\n",
    "# for when df_input[required_features] is called.\n",
    "# For maximum clarity and safety, it's often best to explicitly define and use\n",
    "# these feature lists both during joblib creation and in the Cloud Function.\n",
    "\n",
    "# --- Laptop Model: Training and Saving ---\n",
    "if df_laptop_data is not None:\n",
    "    print(\"\\n\\n=== EXAMPLE 1: TRAINING AND SAVING LAPTOP MODEL (WITH LOG TRANSFORM) ===\")\n",
    "    # IMPORTANT: Set apply_log_transform_to_target to True or False\n",
    "    # to match how MODEL_TRAINED_ON_LOG_TARGET is set in your main.py\n",
    "    APPLY_LOG_LAPTOP = True # Example: Set to True if main.py's constant is True\n",
    "\n",
    "    trained_laptop_pipeline_for_joblib = train_evaluate_lgbm_model(\n",
    "        input_df=df_laptop_data,\n",
    "        target_col_name='precio_mean',\n",
    "        model_name_suffix=\"Laptop_Joblib_Export\",\n",
    "        apply_log_transform_to_target=APPLY_LOG_LAPTOP,\n",
    "        user_feature_cols=user_laptop_features_for_joblib # Pass the specific feature list\n",
    "        # cols_to_always_drop=['titulo', 'precio_min', 'precio_max', 'tipo'] # Default, adjust if needed\n",
    "        # lgbm_hyperparams={...} # Add specific hyperparameters if you tuned them\n",
    "    )\n",
    "    if trained_laptop_pipeline_for_joblib:\n",
    "        print(\"Trained Laptop pipeline for joblib successfully.\")\n",
    "        joblib.dump(trained_laptop_pipeline_for_joblib, 'laptop_model_pipeline.joblib')\n",
    "        print(f\"SUCCESS: 'laptop_model_pipeline.joblib' created. Log transform was: {APPLY_LOG_LAPTOP}\")\n",
    "    else:\n",
    "        print(\"ERROR: Laptop model training for joblib failed. File not created.\")\n",
    "\n",
    "# --- Desktop Model: Training and Saving ---\n",
    "if df_desktop_data is not None:\n",
    "    print(\"\\n\\n=== EXAMPLE 2: TRAINING AND SAVING DESKTOP_PC MODEL ===\")\n",
    "    # IMPORTANT: Set apply_log_transform_to_target to True or False\n",
    "    # to match how MODEL_TRAINED_ON_LOG_TARGET is set in your main.py\n",
    "    APPLY_LOG_DESKTOP = True # Example: Set to True if main.py's constant is True\n",
    "\n",
    "    trained_desktop_pipeline_for_joblib = train_evaluate_lgbm_model(\n",
    "        input_df=df_desktop_data,\n",
    "        target_col_name='precio_mean',\n",
    "        model_name_suffix=\"Desktop_PC_Joblib_Export\",\n",
    "        apply_log_transform_to_target=APPLY_LOG_DESKTOP,\n",
    "        user_feature_cols=user_desktop_features_for_joblib # Pass the specific feature list\n",
    "        # cols_to_always_drop=['titulo', 'precio_min', 'precio_max', 'tipo'] # Default, adjust if needed\n",
    "        # lgbm_hyperparams={...} # Add specific hyperparameters if you tuned them\n",
    "    )\n",
    "    if trained_desktop_pipeline_for_joblib:\n",
    "        print(\"Trained Desktop_PC pipeline for joblib successfully.\")\n",
    "        joblib.dump(trained_desktop_pipeline_for_joblib, 'desktop_model_pipeline.joblib')\n",
    "        print(f\"SUCCESS: 'desktop_model_pipeline.joblib' created. Log transform was: {APPLY_LOG_DESKTOP}\")\n",
    "    else:\n",
    "        print(\"ERROR: Desktop_PC model training for joblib failed. File not created.\")\n",
    "\n",
    "# Example 3: Training Partial_PC model WITHOUT log transform (Commented out as before)\n",
    "# ... (your existing commented out code for Partial_PC) ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Error loading desktop PC data: [Errno 2] No such file or directory: '/df_engineered_desktop_pc.csv'\n",
      "Please ensure the file path for df_desktop_pc is correct.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/df_engineered_desktop_pc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# !!! IMPORTANT: Verify and correct this path for your desktop PC data !!!\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     df_desktop_pc \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/df_engineered_desktop_pc.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Or your actual desktop data file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDesktop PC data loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Load other dataframes if they are used elsewhere in your notebook, otherwise they can be removed.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# df = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered.csv')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# df_laptop = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_laptop.csv')\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# df_partial_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_partial_pc.csv')\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/df_engineered_desktop_pc.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb # <--- Added XGBoost import\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.ensemble import GradientBoostingRegressor # Kept if you might switch back\n",
    "from sklearn.metrics import mean_squared_error, r2_score # make_scorer is available if needed\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb # Kept for the feature selection part, can be removed if FS model also changes\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # !!! IMPORTANT: Verify and correct this path for your desktop PC data !!!\n",
    "    df_desktop_pc = pd.read_csv('/df_engineered_desktop_pc.csv') # Or your actual desktop data file\n",
    "    print(\"Desktop PC data loaded successfully.\")\n",
    "    # Load other dataframes if they are used elsewhere in your notebook, otherwise they can be removed.\n",
    "    # df = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered.csv')\n",
    "    # df_laptop = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_laptop.csv')\n",
    "    # df_partial_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_partial_pc.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading desktop PC data: {e}\")\n",
    "    print(\"Please ensure the file path for df_desktop_pc is correct.\")\n",
    "    raise\n",
    "\n",
    "# --- 2. INITIAL DATA CLEANING (Example from your snippets) ---\n",
    "# Adjust if these columns are not relevant or if other cleaning is needed for df_desktop_pc\n",
    "columns_to_drop = ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
    "existing_cols_desktop = [col for col in columns_to_drop if col in df_desktop_pc.columns]\n",
    "if existing_cols_desktop:\n",
    "    df_desktop_pc = df_desktop_pc.drop(columns=existing_cols_desktop)\n",
    "    print(f\"Dropped columns from df_desktop_pc (if they existed): {existing_cols_desktop}\")\n",
    "\n",
    "\n",
    "# --- 3. SANITY CHECKS FOR df_desktop_pc ---\n",
    "if 'precio_mean' not in df_desktop_pc.columns:\n",
    "    raise ValueError(\"Target column 'precio_mean' not found in df_desktop_pc after initial processing.\")\n",
    "if df_desktop_pc.empty:\n",
    "    raise ValueError(\"The DataFrame df_desktop_pc is empty. Cannot train the model.\")\n",
    "\n",
    "# --- 4. DEFINE INITIAL FEATURES FOR SELECTION ---\n",
    "initial_columns_to_consider = [col for col in df_desktop_pc.columns if col != 'precio_mean']\n",
    "if not initial_columns_to_consider:\n",
    "    raise ValueError(\"No feature columns found in df_desktop_pc after excluding 'precio_mean'.\")\n",
    "print(f\"Initial columns for feature selection consideration: {initial_columns_to_consider}\")\n",
    "\n",
    "# --- 5. FEATURE SELECTION BLOCK (using LightGBM for importances, can be changed) ---\n",
    "# This block remains the same, using LightGBM to determine feature importances.\n",
    "# If you want to use XGBoost for feature selection as well, this part would need modification.\n",
    "print(\"\\n--- Starting Feature Selection Process (using LightGBM for importances) ---\")\n",
    "\n",
    "X_initial = df_desktop_pc[initial_columns_to_consider]\n",
    "y_initial = df_desktop_pc['precio_mean']\n",
    "\n",
    "categorical_features_initial = X_initial.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features_initial = X_initial.select_dtypes(include=['number']).columns\n",
    "\n",
    "print(f\"Initial categorical features for FS: {list(categorical_features_initial)}\")\n",
    "print(f\"Initial numerical features for FS: {list(numerical_features_initial)}\")\n",
    "\n",
    "numerical_transformer_fs = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer_fs = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor_fs = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer_fs, numerical_features_initial),\n",
    "        ('cat', categorical_transformer_fs, categorical_features_initial)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "X_train_fs, _, y_train_fs, _ = train_test_split(X_initial, y_initial, test_size=0.2, random_state=42)\n",
    "\n",
    "lgbm_fs = lgb.LGBMRegressor(random_state=42, verbose=-1) # Using LightGBM for feature selection\n",
    "pipeline_fs = Pipeline(steps=[('preprocessor', preprocessor_fs), ('regressor', lgbm_fs)])\n",
    "\n",
    "print(\"Fitting pipeline for feature selection (LightGBM)...\")\n",
    "pipeline_fs.fit(X_train_fs, y_train_fs)\n",
    "print(\"Feature selection pipeline fitting complete.\")\n",
    "\n",
    "importances_fs = pipeline_fs.named_steps['regressor'].feature_importances_\n",
    "transformed_feature_names_fs = pipeline_fs.named_steps['preprocessor'].get_feature_names_out()\n",
    "original_feature_importances = {col: 0.0 for col in initial_columns_to_consider}\n",
    "\n",
    "for i, full_transformed_name in enumerate(transformed_feature_names_fs):\n",
    "    importance_value = importances_fs[i]\n",
    "    name_parts = full_transformed_name.split('__', 1)\n",
    "    if len(name_parts) < 2: continue\n",
    "    transformer_prefix, internal_name = name_parts\n",
    "    if transformer_prefix == 'num' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "    elif transformer_prefix == 'cat':\n",
    "        for original_cat_col in categorical_features_initial:\n",
    "            if internal_name == original_cat_col or internal_name.startswith(original_cat_col + \"_\"):\n",
    "                original_feature_importances[original_cat_col] += importance_value\n",
    "                break\n",
    "    elif transformer_prefix == 'remainder' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "\n",
    "sorted_original_features = sorted(original_feature_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"\\nFeature importances (aggregated from initial set using LightGBM):\")\n",
    "for feature, score in sorted_original_features: print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "columns_to_keep = [feature for feature, score in sorted_original_features if score > 0]\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nWarning: No features found with importance score > 0 from LightGBM FS.\")\n",
    "else:\n",
    "    print(f\"\\nSelected {len(columns_to_keep)} features (importance > 0 from LightGBM FS) for main model: {columns_to_keep}\")\n",
    "print(\"--- Feature Selection Process Complete ---\")\n",
    "\n",
    "\n",
    "# --- 6. MAIN MODEL TRAINING AND EVALUATION with XGBoost ---\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nHalting script: No features selected for the main model.\")\n",
    "else:\n",
    "    print(\"\\n--- Starting Main Model Training and Evaluation with XGBoost ---\")\n",
    "    X = df_desktop_pc[columns_to_keep]\n",
    "    y = df_desktop_pc['precio_mean']\n",
    "\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "    print(f\"Categorical features for XGBoost model: {list(categorical_features)}\")\n",
    "    print(f\"Numerical features for XGBoost model: {list(numerical_features)}\")\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ], remainder='passthrough')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # XGBoost Parameter Grid for GridSearchCV\n",
    "    # Note: XGBoost can handle categorical features with its 'enable_categorical=True' experimental feature,\n",
    "    # but it's often more robust to one-hot encode them as we are doing.\n",
    "    param_grid_xgb = {\n",
    "        'regressor__n_estimators': [100, 200, 300],\n",
    "        # 'regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "        # 'regressor__max_depth': [3, 5, 7], # Typical values for XGBoost\n",
    "        # 'regressor__subsample': [0.7, 0.8, 1.0],\n",
    "        # 'regressor__colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        # 'regressor__gamma': [0, 0.1, 0.2], # Minimum loss reduction required to make a further partition\n",
    "        # 'regressor__reg_alpha': [0, 0.01, 0.1], # L1 regularization\n",
    "        # 'regressor__reg_lambda': [1, 0.1, 0.01]  # L2 regularization (XGBoost default is 1)\n",
    "    }\n",
    "\n",
    "    # Create the pipeline with XGBoost Regressor\n",
    "    # XGBoost may issue warnings about unsupported 'verbose' if passed from LGBM feature selector;\n",
    "    # we define a new regressor instance here.\n",
    "    pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', xgb.XGBRegressor(random_state=42, objective='reg:squarederror'))])\n",
    "                                   # objective='reg:squarederror' suppresses a warning in newer XGBoost versions.\n",
    "\n",
    "    grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "    print(\"Fitting GridSearchCV with XGBoost...\")\n",
    "    grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nBest parameters found by GridSearchCV for XGBoost:\")\n",
    "    print(grid_search_xgb.best_params_)\n",
    "\n",
    "    best_model_xgb = grid_search_xgb.best_estimator_\n",
    "    print(\"\\nXGBoost model pipeline fitting complete using best estimator from GridSearchCV.\")\n",
    "\n",
    "    # Evaluate on the Test Set\n",
    "    y_pred_test_xgb = best_model_xgb.predict(X_test)\n",
    "    test_mse_xgb = mean_squared_error(y_test, y_pred_test_xgb)\n",
    "    test_rmse_xgb = np.sqrt(test_mse_xgb)\n",
    "    test_r2_xgb = r2_score(y_test, y_pred_test_xgb)\n",
    "\n",
    "    print(\"\\nMetrics on the TEST Set (using best XGBoost model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {test_mse_xgb:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {test_rmse_xgb:.4f}\")\n",
    "    print(f\"R-squared: {test_r2_xgb:.4f}\")\n",
    "\n",
    "    # Evaluate on the Training Set\n",
    "    y_pred_train_xgb = best_model_xgb.predict(X_train)\n",
    "    train_mse_xgb = mean_squared_error(y_train, y_pred_train_xgb)\n",
    "    train_rmse_xgb = np.sqrt(train_mse_xgb)\n",
    "    train_r2_xgb = r2_score(y_train, y_pred_train_xgb)\n",
    "\n",
    "    print(\"\\nMetrics on the TRAINING Set (using best XGBoost model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {train_mse_xgb:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {train_rmse_xgb:.4f}\")\n",
    "    print(f\"R-squared: {train_r2_xgb:.4f}\")\n",
    "\n",
    "print(\"\\n--- Script Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Desktop PC data loaded successfully.\n",
      "Dropped columns from df_desktop_pc (if they existed): ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
      "Initial columns for feature selection: ['tipo_de_producto', 'serie', 'procesador', 'disco_duro_tipo_de_disco_duro', 'procesador_fabricante', 'procesador_tipo', 'disco_duro_numero_de_discos_duros_instalados', 'sistema_operativo_sistema_operativo', 'procesador_nombre_clave', 'ram_tipo', 'procesador_numero_nucleos', 'grafica_tarjeta', 'comunicaciones_estandar_lan', 'propiedades_de_la_carcasa_tipo_de_caja', 'adecuado_para', 'procesador_zocalo_de_cpu', 'medidas_y_peso_profundidad_cm', 'medidas_y_peso_ancho_cm', 'procesador_cache_mb', 'ram_memoria_gb', 'ram_frecuencia_de_la_memoria_mhz', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador_frecuencia_turbo_max_ghz', 'medidas_y_peso_alto_cm', 'procesador_frecuencia_del_reloj_ghz', 'procesador_tdp_watts', 'grafica_memoria_mb', 'alimentacion_wattage_binned', 'custom_category', 'equip_altavoces_estéreo', 'equip_refrigeración_líquida', 'equip_usb_c']\n",
      "\n",
      "--- Starting Feature Selection Process (using LightGBM for importances) ---\n",
      "Fitting pipeline for feature selection (LightGBM)...\n",
      "Feature selection pipeline fitting complete.\n",
      "\n",
      "Feature importances (aggregated from LightGBM FS):\n",
      "procesador: 339.0000\n",
      "medidas_y_peso_profundidad_cm: 299.0000\n",
      "serie: 262.0000\n",
      "grafica_tarjeta: 244.0000\n",
      "medidas_y_peso_ancho_cm: 191.0000\n",
      "tipo_de_producto: 178.0000\n",
      "ram_memoria_gb: 172.0000\n",
      "grafica_memoria_mb: 154.0000\n",
      "medidas_y_peso_alto_cm: 148.0000\n",
      "propiedades_de_la_carcasa_tipo_de_caja: 129.0000\n",
      "procesador_frecuencia_del_reloj_ghz: 123.0000\n",
      "procesador_frecuencia_turbo_max_ghz: 121.0000\n",
      "procesador_cache_mb: 105.0000\n",
      "disco_duro_capacidad_de_memoria_ssd_gb: 101.0000\n",
      "sistema_operativo_sistema_operativo: 78.0000\n",
      "procesador_numero_nucleos: 66.0000\n",
      "alimentacion_wattage_binned: 57.0000\n",
      "procesador_tdp_watts: 54.0000\n",
      "adecuado_para: 46.0000\n",
      "ram_tipo: 42.0000\n",
      "comunicaciones_estandar_lan: 33.0000\n",
      "ram_frecuencia_de_la_memoria_mhz: 32.0000\n",
      "disco_duro_tipo_de_disco_duro: 16.0000\n",
      "disco_duro_numero_de_discos_duros_instalados: 5.0000\n",
      "equip_refrigeración_líquida: 5.0000\n",
      "procesador_fabricante: 0.0000\n",
      "procesador_tipo: 0.0000\n",
      "procesador_nombre_clave: 0.0000\n",
      "procesador_zocalo_de_cpu: 0.0000\n",
      "custom_category: 0.0000\n",
      "equip_altavoces_estéreo: 0.0000\n",
      "equip_usb_c: 0.0000\n",
      "\n",
      "Selected 25 features (importance > 0 from LightGBM FS) for main model: ['procesador', 'medidas_y_peso_profundidad_cm', 'serie', 'grafica_tarjeta', 'medidas_y_peso_ancho_cm', 'tipo_de_producto', 'ram_memoria_gb', 'grafica_memoria_mb', 'medidas_y_peso_alto_cm', 'propiedades_de_la_carcasa_tipo_de_caja', 'procesador_frecuencia_del_reloj_ghz', 'procesador_frecuencia_turbo_max_ghz', 'procesador_cache_mb', 'disco_duro_capacidad_de_memoria_ssd_gb', 'sistema_operativo_sistema_operativo', 'procesador_numero_nucleos', 'alimentacion_wattage_binned', 'procesador_tdp_watts', 'adecuado_para', 'ram_tipo', 'comunicaciones_estandar_lan', 'ram_frecuencia_de_la_memoria_mhz', 'disco_duro_tipo_de_disco_duro', 'disco_duro_numero_de_discos_duros_instalados', 'equip_refrigeración_líquida']\n",
      "--- Feature Selection Process Complete ---\n",
      "\n",
      "--- Starting Main Model Training and Evaluation with MLPRegressor ---\n",
      "Categorical features for MLP model: ['procesador', 'serie', 'grafica_tarjeta', 'tipo_de_producto', 'propiedades_de_la_carcasa_tipo_de_caja', 'sistema_operativo_sistema_operativo', 'alimentacion_wattage_binned', 'adecuado_para', 'ram_tipo', 'comunicaciones_estandar_lan', 'disco_duro_tipo_de_disco_duro']\n",
      "Numerical features for MLP model: ['medidas_y_peso_profundidad_cm', 'medidas_y_peso_ancho_cm', 'ram_memoria_gb', 'grafica_memoria_mb', 'medidas_y_peso_alto_cm', 'procesador_frecuencia_del_reloj_ghz', 'procesador_frecuencia_turbo_max_ghz', 'procesador_cache_mb', 'disco_duro_capacidad_de_memoria_ssd_gb', 'procesador_numero_nucleos', 'procesador_tdp_watts', 'ram_frecuencia_de_la_memoria_mhz', 'disco_duro_numero_de_discos_duros_instalados', 'equip_refrigeración_líquida']\n",
      "Fitting GridSearchCV with MLPRegressor...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found by GridSearchCV for MLPRegressor:\n",
      "{'regressor__alpha': 0.01, 'regressor__early_stopping': True, 'regressor__hidden_layer_sizes': (50, 25)}\n",
      "\n",
      "MLPRegressor model pipeline fitting complete using best estimator.\n",
      "\n",
      "Metrics on the TEST Set (using best MLPRegressor model):\n",
      "Mean Squared Error (MSE): 1000189.1784\n",
      "Root Mean Squared Error (RMSE): 1000.0946\n",
      "R-squared: 0.5298\n",
      "\n",
      "Metrics on the TRAINING Set (using best MLPRegressor model):\n",
      "Mean Squared Error (MSE): 237079.8472\n",
      "Root Mean Squared Error (RMSE): 486.9085\n",
      "R-squared: 0.8259\n",
      "\n",
      "--- Script Execution Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor # <--- Added MLPRegressor import\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb # Kept for the feature selection part\n",
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    # !!! IMPORTANT: Verify and correct this path for your desktop PC data !!!\n",
    "    df_desktop_pc = pd.read_csv('/Users/oliverholmes/Documents/BCSAI/SecondYear/Machine Learning/Assignments/PcPartPicker3000/assignment/df_engineered_desktop_pc.csv') # Or your actual desktop data file\n",
    "    print(\"Desktop PC data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading desktop PC data: {e}\")\n",
    "    print(\"Please ensure the file path for df_desktop_pc is correct.\")\n",
    "    raise\n",
    "\n",
    "# --- 2. INITIAL DATA CLEANING ---\n",
    "columns_to_drop = ['titulo', 'precio_min', 'precio_max', 'tipo']\n",
    "existing_cols_desktop = [col for col in columns_to_drop if col in df_desktop_pc.columns]\n",
    "if existing_cols_desktop:\n",
    "    df_desktop_pc = df_desktop_pc.drop(columns=existing_cols_desktop)\n",
    "    print(f\"Dropped columns from df_desktop_pc (if they existed): {existing_cols_desktop}\")\n",
    "\n",
    "# --- 3. SANITY CHECKS FOR df_desktop_pc ---\n",
    "if 'precio_mean' not in df_desktop_pc.columns:\n",
    "    raise ValueError(\"Target column 'precio_mean' not found in df_desktop_pc.\")\n",
    "if df_desktop_pc.empty:\n",
    "    raise ValueError(\"The DataFrame df_desktop_pc is empty.\")\n",
    "\n",
    "# --- 4. DEFINE INITIAL FEATURES FOR SELECTION ---\n",
    "initial_columns_to_consider = [col for col in df_desktop_pc.columns if col != 'precio_mean']\n",
    "if not initial_columns_to_consider:\n",
    "    raise ValueError(\"No feature columns found in df_desktop_pc after excluding 'precio_mean'.\")\n",
    "print(f\"Initial columns for feature selection: {initial_columns_to_consider}\")\n",
    "\n",
    "# --- 5. FEATURE SELECTION BLOCK (using LightGBM for importances) ---\n",
    "print(\"\\n--- Starting Feature Selection Process (using LightGBM for importances) ---\")\n",
    "X_initial = df_desktop_pc[initial_columns_to_consider]\n",
    "y_initial = df_desktop_pc['precio_mean']\n",
    "\n",
    "categorical_features_initial = X_initial.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features_initial = X_initial.select_dtypes(include=['number']).columns\n",
    "\n",
    "numerical_transformer_fs = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer_fs = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "preprocessor_fs = ColumnTransformer(transformers=[('num', numerical_transformer_fs, numerical_features_initial), ('cat', categorical_transformer_fs, categorical_features_initial)], remainder='passthrough')\n",
    "\n",
    "X_train_fs, _, y_train_fs, _ = train_test_split(X_initial, y_initial, test_size=0.2, random_state=42)\n",
    "lgbm_fs = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "pipeline_fs = Pipeline(steps=[('preprocessor', preprocessor_fs), ('regressor', lgbm_fs)])\n",
    "\n",
    "print(\"Fitting pipeline for feature selection (LightGBM)...\")\n",
    "pipeline_fs.fit(X_train_fs, y_train_fs)\n",
    "print(\"Feature selection pipeline fitting complete.\")\n",
    "\n",
    "importances_fs = pipeline_fs.named_steps['regressor'].feature_importances_\n",
    "transformed_feature_names_fs = pipeline_fs.named_steps['preprocessor'].get_feature_names_out()\n",
    "original_feature_importances = {col: 0.0 for col in initial_columns_to_consider}\n",
    "\n",
    "for i, full_transformed_name in enumerate(transformed_feature_names_fs):\n",
    "    importance_value = importances_fs[i]\n",
    "    name_parts = full_transformed_name.split('__', 1)\n",
    "    if len(name_parts) < 2: continue\n",
    "    transformer_prefix, internal_name = name_parts\n",
    "    if transformer_prefix == 'num' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "    elif transformer_prefix == 'cat':\n",
    "        for original_cat_col in categorical_features_initial:\n",
    "            if internal_name == original_cat_col or internal_name.startswith(original_cat_col + \"_\"):\n",
    "                original_feature_importances[original_cat_col] += importance_value\n",
    "                break\n",
    "    elif transformer_prefix == 'remainder' and internal_name in original_feature_importances:\n",
    "        original_feature_importances[internal_name] += importance_value\n",
    "\n",
    "sorted_original_features = sorted(original_feature_importances.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"\\nFeature importances (aggregated from LightGBM FS):\")\n",
    "for feature, score in sorted_original_features: print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "columns_to_keep = [feature for feature, score in sorted_original_features if score > 0]\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nWarning: No features found with importance > 0 from LightGBM FS.\")\n",
    "else:\n",
    "    print(f\"\\nSelected {len(columns_to_keep)} features (importance > 0 from LightGBM FS) for main model: {columns_to_keep}\")\n",
    "print(\"--- Feature Selection Process Complete ---\")\n",
    "\n",
    "# --- 6. MAIN MODEL TRAINING AND EVALUATION with MLPRegressor ---\n",
    "if not columns_to_keep:\n",
    "    print(\"\\nHalting script: No features selected for the main model.\")\n",
    "else:\n",
    "    print(\"\\n--- Starting Main Model Training and Evaluation with MLPRegressor ---\")\n",
    "    X = df_desktop_pc[columns_to_keep]\n",
    "    y = df_desktop_pc['precio_mean']\n",
    "\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "    print(f\"Categorical features for MLP model: {list(categorical_features)}\")\n",
    "    print(f\"Numerical features for MLP model: {list(numerical_features)}\")\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]) # Scaling is crucial for NNs\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_features), ('cat', categorical_transformer, categorical_features)], remainder='passthrough')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # MLPRegressor Parameter Grid for GridSearchCV\n",
    "    param_grid_mlp = {\n",
    "        'regressor__hidden_layer_sizes': [(50,), (100,), (50,25)], # One or two hidden layers\n",
    "        # 'regressor__activation': ['relu', 'tanh'],\n",
    "        # 'regressor__solver': ['adam'], # Adam is often a good default\n",
    "        'regressor__alpha': [0.0001, 0.001, 0.01], # L2 regularization\n",
    "        # 'regressor__learning_rate_init': [0.001, 0.01],\n",
    "        # 'regressor__max_iter': [300, 500] # Allow more iterations for convergence\n",
    "        'regressor__early_stopping': [True], # Can help prevent overfitting and speed up grid search\n",
    "        # 'regressor__n_iter_no_change': [10]   # Used with early_stopping\n",
    "    }\n",
    "\n",
    "    pipeline_mlp = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('regressor', MLPRegressor(random_state=42))])\n",
    "\n",
    "    grid_search_mlp = GridSearchCV(pipeline_mlp, param_grid_mlp, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "    print(\"Fitting GridSearchCV with MLPRegressor...\")\n",
    "    grid_search_mlp.fit(X_train, y_train) # NNs can take longer to train, especially with GridSearchCV\n",
    "\n",
    "    print(\"\\nBest parameters found by GridSearchCV for MLPRegressor:\")\n",
    "    print(grid_search_mlp.best_params_)\n",
    "\n",
    "    best_model_mlp = grid_search_mlp.best_estimator_\n",
    "    print(\"\\nMLPRegressor model pipeline fitting complete using best estimator.\")\n",
    "\n",
    "    # Evaluate on the Test Set\n",
    "    y_pred_test_mlp = best_model_mlp.predict(X_test)\n",
    "    test_mse_mlp = mean_squared_error(y_test, y_pred_test_mlp)\n",
    "    test_rmse_mlp = np.sqrt(test_mse_mlp)\n",
    "    test_r2_mlp = r2_score(y_test, y_pred_test_mlp)\n",
    "\n",
    "    print(\"\\nMetrics on the TEST Set (using best MLPRegressor model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {test_mse_mlp:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {test_rmse_mlp:.4f}\")\n",
    "    print(f\"R-squared: {test_r2_mlp:.4f}\")\n",
    "\n",
    "    # Evaluate on the Training Set\n",
    "    y_pred_train_mlp = best_model_mlp.predict(X_train)\n",
    "    train_mse_mlp = mean_squared_error(y_train, y_pred_train_mlp)\n",
    "    train_rmse_mlp = np.sqrt(train_mse_mlp)\n",
    "    train_r2_mlp = r2_score(y_train, y_pred_train_mlp)\n",
    "\n",
    "    print(\"\\nMetrics on the TRAINING Set (using best MLPRegressor model):\")\n",
    "    print(f\"Mean Squared Error (MSE): {train_mse_mlp:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {train_rmse_mlp:.4f}\")\n",
    "    print(f\"R-squared: {train_r2_mlp:.4f}\")\n",
    "\n",
    "print(\"\\n--- Script Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 230 minutes of running, this was the result of the cross validation:\n",
    "\n",
    "GridSearchCV Complete.\n",
    "Best parameters found: {'regressor__colsample_bytree': 0.8, 'regressor__learning_rate': 0.05, 'regressor__max_depth': 10, 'regressor__min_child_samples': 20, 'regressor__n_estimators': 500, 'regressor__num_leaves': 70, 'regressor__subsample': 0.8}\n",
    "\n",
    "Best cross-validation score (negative MSE): -375894.4673686046\n",
    "\n",
    "Best cross-validation RMSE: 613.1023302586646\n",
    "\n",
    "Metrics on the Test Set using the Best Model from GridSearchCV:\n",
    "\n",
    "Mean Squared Error (MSE): 347437.5443584199\n",
    "\n",
    "Root Mean Squared Error (RMSE): 589.4383295633394\n",
    "\n",
    "R-squared: 0.7957941987120332\n",
    "Metrics on the Training Set using the Best Model from GridSearchCV:\n",
    "\n",
    "Mean Squared Error (MSE): 73745.4619638589\n",
    "\n",
    "Root Mean Squared Error (RMSE): 271.5611569496987\n",
    "\n",
    "R-squared: 0.9516823528897075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
